{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e761df69-3ea9-443e-88c6-87ced0d3f9df",
   "metadata": {},
   "source": [
    "# Examen del 2do Parcial\n",
    "Rolando Jiménez Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac53475-ca81-4a07-82e5-20e01a1787f0",
   "metadata": {},
   "source": [
    "1. Utilizando el dataset de _Coactivation_matrix.mat_:\n",
    "- Calcule el coeficiente de mundo pequeño\n",
    "- Calcule las comunidades del grafo\n",
    "- Calcule los hub\n",
    "- Calcule la eficiencia global\n",
    "- Calcule la eficiencia local\n",
    "- Determine el grado de cada nodo\n",
    "\n",
    "2. Utilizando el dataset de _Coactivation_matrix.mat_:\n",
    "- Generar el mapa de calor de cada matriz de conectividad\n",
    "- Generar la distribución de datos de cada matriz de conectividad\n",
    "- ¿Qué valor se encuentra en el percentil 0.25, 0.5 y 0.75 de la matriz de conectividad?\n",
    "- Cree los 3 grafos 2D filtrando la matriz de conectividad con los valores dados por los percentiles del ejercicio anterior\n",
    "\n",
    "3. Utilizando el dataset de _Coactivation_matrix.mat_:\n",
    "- Genere el grafo 3D\n",
    "- Haga que el tamaño de nodos sea proporcional a su grado; es decir, q entre mayor sea su grado, mayor sea el tamaño del nodo ploteado\n",
    "- Haga que el color de las aristas este relacionado al valor de la matriz de conectividad. Utilizar el map color Hot\n",
    "\n",
    "4. Utilizando el dataset de _chb01_01.edf_:\n",
    "- Calcule el coeficiente de mundo pequeño\n",
    "- Calcule las comunidades del grafo\n",
    "- Calcule los hub\n",
    "- Calcule la eficiencia global\n",
    "- Calcule la eficiencia local\n",
    "- Determine el grado de cada nodo\n",
    "\n",
    "5. Utilizando el dataset de _chb01_01.edf_:\n",
    "- Genere el grafo 3D\n",
    "- Haga que el tamaño de nodos sea proporcional a su grado; es decir, q entre mayor sea su grado, mayor sea el tamaño del nodo ploteado\n",
    "- Haga que el color de las aristas este relacionado al valor de la matriz de conectividad. Utilizar el map color Hot\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb3677d6-3bf0-4e33-a625-e66df2ad1c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T22:52:09.855207Z",
     "start_time": "2025-10-27T22:52:07.812446Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from scipy.signal import coherence, hilbert\n",
    "import scipy.io"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EJERCICIO 1",
   "id": "90e972b3e90133de"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-27T22:52:11.550557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "Mat_Coact = 'C:/Users/rayom/OneDrive/Documentos/GitHub/Neurociencias-2026-1/S03_datasets/BCT/Coactivation_matrix.mat'\n",
    "datos = scipy.io.loadmat(Mat_Coact)\n",
    "data = datos[\"Coactivation_matrix\"]\n",
    "G = nx.from_numpy_array(data)\n",
    "C = nx.average_clustering(G)\n",
    "L = nx.average_shortest_path_length(G)\n",
    "C, L\n",
    "print(C)\n",
    "print(L)\n",
    "\n",
    "#Comunidades\n",
    "from networkx.algorithms import community\n",
    "\n",
    "comunidades = list(community.greedy_modularity_communities(G))\n",
    "print(f\"Número de comunidades: {len(comunidades)}\")\n",
    "\n",
    "for i, c in enumerate(comunidades):\n",
    "    print(f\"Comunidad {i+1} ({len(c)} nodos): {sorted(list(c))[:10]}\")\n",
    "\n",
    "#Hubs\n",
    "degrees = dict(G.degree())\n",
    "avg_degree = np.mean(list(degrees.values()))\n",
    "hubs = [node for node, degree in degrees.items() if degree > avg_degree]\n",
    "print(\"Potential Hubs (degree > average):\", hubs)\n",
    "\n",
    "# Eficiencia Global\n",
    "def global_efficiency(G):\n",
    "    n = len(G)\n",
    "    if n <= 1:\n",
    "        return 0\n",
    "    lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    s = 0\n",
    "    for u in lengths:\n",
    "        for v, d in lengths[u].items():\n",
    "            if u != v:\n",
    "                s += 1 / d\n",
    "    return s / (n * (n - 1))\n",
    "\n",
    "#Eficencia Local\n",
    "def local_efficiency(G):\n",
    "    effs = []\n",
    "    for v in G:\n",
    "        vecinos = list(G.neighbors(v))\n",
    "        if len(vecinos) >= 2:\n",
    "            sub = G.subgraph(vecinos)\n",
    "            effs.append(global_efficiency(sub))\n",
    "    return np.mean(effs)\n",
    "\n",
    "E_global = global_efficiency(G)\n",
    "E_local = local_efficiency(G)\n",
    "\n",
    "print(f\"Eficiencia global: {E_global:.3f}\")\n",
    "print(f\"Eficiencia local:  {E_local:.3f}\")\n",
    "\n",
    "# Grado de cada nodo\n",
    "\n",
    "degree_dict = dict(G.degree())\n",
    "degree_dict\n"
   ],
   "id": "e29e060ae9a83da0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3844533292242755\n",
      "2.2148737961545844\n",
      "Número de comunidades: 3\n",
      "Comunidad 1 (334 nodos): [0, 1, 2, 3, 4, 8, 9, 11, 13, 14]\n",
      "Comunidad 2 (201 nodos): [5, 6, 7, 10, 12, 15, 16, 18, 20, 37]\n",
      "Comunidad 3 (103 nodos): [26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "Potential Hubs (degree > average): [6, 7, 11, 16, 18, 19, 20, 22, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 50, 62, 63, 65, 68, 69, 70, 73, 76, 79, 80, 87, 93, 95, 97, 98, 100, 103, 104, 116, 118, 120, 121, 124, 125, 126, 128, 129, 130, 131, 135, 154, 158, 160, 167, 184, 186, 193, 194, 197, 202, 208, 211, 217, 219, 220, 223, 228, 230, 231, 232, 233, 235, 237, 238, 240, 244, 248, 252, 253, 259, 260, 261, 262, 263, 266, 267, 268, 269, 270, 271, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 291, 292, 296, 298, 303, 305, 308, 309, 327, 328, 329, 330, 331, 332, 333, 334, 335, 339, 344, 345, 346, 347, 348, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 364, 365, 367, 368, 369, 370, 371, 373, 374, 375, 377, 383, 385, 387, 391, 393, 394, 396, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 410, 411, 412, 414, 416, 418, 419, 420, 421, 427, 428, 431, 434, 436, 447, 450, 452, 453, 455, 457, 458, 462, 463, 465, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 485, 487, 488, 491, 493, 494, 495, 496, 497, 500, 501, 502, 503, 504, 505, 506, 507, 509, 510, 511, 513, 521, 532, 536, 544, 546, 549, 552, 557, 559, 564, 580, 590, 598, 599, 600, 602, 604, 605, 607, 610, 611, 612, 614, 619, 620, 621, 622, 623, 628, 629, 630, 631]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EJERCICIO 2",
   "id": "d48de34d9cacb630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando 638 canales y 638 muestras...\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "try:\n",
    "    Mat_Coact = 'C:/Users/rayom/OneDrive/Documentos/GitHub/Neurociencias-2026-1/S03_datasets/BCT/Coactivation_matrix.mat'\n",
    "    datos = scipy.io.loadmat(Mat_Coact)\n",
    "    data = datos[\"Coactivation_matrix\"]\n",
    "\n",
    "    if data.shape[0] < data.shape[1]:\n",
    "        data = data.T\n",
    "        print(f\"Datos cargados y transpuestos a formato (N_canales={data.shape[0]}, N_muestras={data.shape[1]})\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Asegúrate de que 'Coactivation_matrix.mat' esté en el directorio correcto.\")\n",
    "\n",
    "# Parámetros de la señal\n",
    "sfreq_target = 256\n",
    "fmin, fmax = 8, 13\n",
    "n_channels = data.shape[0]\n",
    "n_samples = data.shape[1]\n",
    "\n",
    "print(f\"Analizando {n_channels} canales y {n_samples} muestras...\")\n",
    "\n",
    "#Matrices de conectividad\n",
    "\n",
    "# Pearson\n",
    "corr_matrix = np.corrcoef(data)\n",
    "corr_df = pd.DataFrame(corr_matrix)\n",
    "\n",
    "#Coherencia\n",
    "coh_matrix = np.zeros((n_channels, n_channels))\n",
    "for i in range(n_channels):\n",
    "    for j in range(i, n_channels):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            f, Cxy = coherence(data[i], data[j], fs=sfreq_target, nperseg=sfreq_target * 2)\n",
    "\n",
    "        mask = (f >= fmin) & (f <= fmax)\n",
    "\n",
    "        coh_val = np.mean(Cxy[mask])\n",
    "        if np.isnan(coh_val):\n",
    "            coh_val = 0.0\n",
    "\n",
    "        coh_matrix[i, j] = coh_val\n",
    "        coh_matrix[j, i] = coh_val\n",
    "\n",
    "# Rellenar la diagonal con 1.0 (coherencia perfecta consigo mismo)\n",
    "np.fill_diagonal(coh_matrix, 1.0)\n",
    "coh_df = pd.DataFrame(coh_matrix)\n",
    "\n",
    "# C. Phase Locking Value (PLV)\n",
    "analytic_signal = hilbert(data)\n",
    "phase_data = np.angle(analytic_signal)\n",
    "\n",
    "plv_matrix = np.zeros((n_channels, n_channels))\n",
    "for i in range(n_channels):\n",
    "    for j in range(i, n_channels): # Optimización\n",
    "        phase_diff = phase_data[i] - phase_data[j]\n",
    "        # PLV es el valor absoluto de la fase media compleja\n",
    "        plv_val = np.abs(np.sum(np.exp(1j * phase_diff)) / phase_diff.size)\n",
    "\n",
    "        plv_matrix[i, j] = plv_val\n",
    "        plv_matrix[j, i] = plv_val # Simetría\n",
    "\n",
    "# Rellenar la diagonal con 1.0\n",
    "np.fill_diagonal(plv_matrix, 1.0)\n",
    "plv_df = pd.DataFrame(plv_matrix)\n",
    "\n",
    "# Diccionario de las matrices para el bucle de análisis\n",
    "matrices = {\n",
    "    'Pearson_Correlation': corr_df,\n",
    "    'Coherence': coh_df,\n",
    "    'PLV': plv_df\n",
    "}"
   ],
   "id": "e467f417b97092e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "resultados_percentiles = {}\n",
    "grafos_filtrados = {}\n",
    "\n",
    "for name, df in matrices.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"INICIANDO ANÁLISIS para: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(df.values, annot=False, cmap='viridis' if name == 'PLV' else 'coolwarm',\n",
    "                fmt=\".2f\", cbar_kws={'label': f'{name} Value'})\n",
    "    plt.title(f'Mapa de Calor: {name} Matrix')\n",
    "    plt.xlabel('Canal J')\n",
    "    plt.ylabel('Canal I')\n",
    "    plt.show()\n",
    "\n",
    "    val_data = df.values[np.triu_indices_from(df, k=1)]"
   ],
   "id": "6df632c604516541",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(val_data, bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribución de Datos: {name} Matrix')\n",
    "    plt.xlabel(f'{name} Value')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(axis='y', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    p25 = np.percentile(val_data, 25)\n",
    "    p50 = np.percentile(val_data, 50)\n",
    "    p75 = np.percentile(val_data, 75)\n",
    "\n",
    "    resultados_percentiles[name] = {'P25': p25, 'P50': p50, 'P75': p75}\n",
    "\n",
    "    print(f\"\\nValores de Percentiles para {name}:\")\n",
    "    print(f\"  Percentil 25 (Q1): {p25:.4f}\")\n",
    "    print(f\"  Percentil 50 (Mediana): {p50:.4f}\")\n",
    "    print(f\"  Percentil 75 (Q3): {p75:.4f}\")\n",
    "\n",
    "\n",
    "    percentiles = {'P25': p25, 'P50': p50, 'P75': p75}\n",
    "\n",
    "    for p_name, threshold in percentiles.items():\n",
    "        G_filt = nx.Graph()\n",
    "        num_nodes = df.shape[0]\n",
    "        G_filt.add_nodes_from(range(num_nodes))\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                weight = df.iloc[i, j]\n",
    "                if abs(weight) > threshold:\n",
    "                    G_filt.add_edge(i, j, weight=weight)\n",
    "\n",
    "        graph_key = f\"{name}_{p_name}\"\n",
    "        grafos_filtrados[graph_key] = G_filt\n",
    "        print(f\"  Grafo '{p_name}': Creado con {G_filt.number_of_edges()} aristas.\")\n"
   ],
   "id": "7e13c8191f117dfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Grafos\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"GENERANDO GRÁFICOS 2D DE LOS 9 GRAFOS FILTRADOS\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "for graph_key, G_to_draw in grafos_filtrados.items():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G_to_draw, seed=42)\n",
    "\n",
    "    edge_count = G_to_draw.number_of_edges()\n",
    "    node_size = 20 if edge_count > 1000 else 50\n",
    "    alpha_val = 0.3 if edge_count > 5000 else 0.6\n",
    "\n",
    "    nx.draw(G_to_draw, pos, with_labels=False, node_size=node_size,\n",
    "            edge_color='gray', alpha=alpha_val, width=0.5)\n",
    "    plt.title(f'Grafo Filtrado ({G_to_draw.number_of_edges()} Aristas): {graph_key.replace(\"_\", \" \").title()}',\n",
    "              fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAnálisis completado.\")"
   ],
   "id": "2d0c059d73f9b359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " #Coherencia espectral\n",
    "plt.figure(figsize=(8, 2))\n",
    "coh = sns.heatmap(coh_df.values,\n",
    "                 annot=False, cmap='GnBu', fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "val_coh = corr_df.values[np.triu_indices_from(coh_df, k=1)]\n",
    "plt.hist(val_coh, bins=45)\n",
    "\n",
    "p25_c = np.percentile(val_coh, 25)\n",
    "p50_c = np.percentile(val_coh, 50)\n",
    "p75_c = np.percentile(val_coh, 75)\n",
    "\n",
    "print(f\"Percentil 25: {p25_c:.4f}\")\n",
    "print(f\"Percentil 50: {p50_c:.4f}\")\n",
    "print(f\"Percentil 75: {p75_c:.4f}\")\n",
    "\n",
    "# Ggráficas\n",
    "percentiles_c = {'25': p25_c, '50': p50_c, '75': p75_c}\n",
    "filtrado_c = {}\n",
    "\n",
    "for name, threshold in percentiles_c.items():\n",
    "    G_filt_c = nx.Graph()\n",
    "    num_nodes_c = coh_df.shape[0]\n",
    "    G_filt_c.add_nodes_from(range(num_nodes_c))\n",
    "\n",
    "    for i in range(num_nodes_c):\n",
    "        for j in range(i + 1, num_nodes_c):\n",
    "            weight_c = coh_df.iloc[i, j]\n",
    "            if abs(weight_c) > threshold:\n",
    "                G_filt_c.add_edge(i, j, weight_c=weight_c)\n",
    "\n",
    "    graph_key_c = f\"coherencia espectral_{name.replace(' ', '_').lower()}\"\n",
    "    filtrado_c[graph_key_c] = G_filt_c\n",
    "\n",
    "    print(f\"Created graph '{graph_key_c}' with {G_filt_c.number_of_nodes()} nodes and {G_filt_c.number_of_edges()} edges.\")\n",
    "\n",
    "for graph_key, G_to_draw in filtrado_c.items():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G_to_draw)\n",
    "    nx.draw(G_to_draw, pos, with_labels=False, node_size=20, edge_color='gray', alpha=0.6)\n",
    "    plt.title(f'Filtered Graph: {graph_key.replace(\"_\", \" \").title()}')\n",
    "    plt.show()"
   ],
   "id": "ce31eb195fa9969c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "plv = sns.heatmap(plv_df.values,\n",
    "                 annot=False, cmap='GnBu', fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "# Distribución\n",
    "plt.figure(figsize=(8, 6))\n",
    "val_plv = corr_df.values[np.triu_indices_from(plv_df, k=1)]\n",
    "plt.hist(val_plv, bins=45)\n",
    "\n",
    "p25_p = np.percentile(val_plv, 25)\n",
    "p50_p = np.percentile(val_plv, 50)\n",
    "p75_p = np.percentile(val_plv, 75)\n",
    "\n",
    "print(f\"Percentil 25: {p25_p:.4f}\")\n",
    "print(f\"Percentil 50: {p50_p:.4f}\")\n",
    "print(f\"Percentil 75: {p75_p:.4f}\")\n",
    "\n",
    "# gráficas\n",
    "percentiles_p = {'25': p25_p, '50': p50_p, '75': p75_p}\n",
    "filtrado_p = {}\n",
    "\n",
    "for name, threshold in percentiles_p.items():\n",
    "    G_filt_p = nx.Graph()\n",
    "    num_nodes_p = coh_df.shape[0]\n",
    "    G_filt_p.add_nodes_from(range(num_nodes_p))\n",
    "\n",
    "    for i in range(num_nodes_p):\n",
    "        for j in range(i + 1, num_nodes_p):\n",
    "            weight_p = plv_df.iloc[i, j]\n",
    "            if abs(weight_p) > threshold:\n",
    "                G_filt_c.add_edge(i, j, weight_p=weight_p)\n",
    "\n",
    "    graph_key_p = f\"phase locking value_{name.replace(' ', '_').lower()}\"\n",
    "    filtrado_p[graph_key_p] = G_filt_p\n",
    "\n",
    "    print(f\"Created graph '{graph_key_p}' with {G_filt_p.number_of_nodes()} nodes and {G_filt_p.number_of_edges()} edges.\")\n",
    "\n",
    "for graph_key, G_to_draw in filtrado_p.items():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G_to_draw)\n",
    "    nx.draw(G_to_draw, pos, with_labels=False, node_size=20, edge_color='gray', alpha=0.6)\n",
    "    plt.title(f'Filtered Graph: {graph_key.replace(\"_\", \" \").title()}')\n",
    "    plt.show()"
   ],
   "id": "fe624af38e18f401",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ejercicio 3",
   "id": "bab13485195a8763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "coords = datos['Coord']\n",
    "\n",
    "G_3d = nx.from_numpy_array(data)\n",
    "\n",
    "degrees_3d = dict(G_3d.degree())\n",
    "node_size_factor = 5\n",
    "node_sizes = [degrees_3d[node] * node_size_factor for node in G_3d.nodes()]\n",
    "\n",
    "edge_weights = np.array([G_3d[u][v]['weight'] for u, v in G_3d.edges()])\n",
    "\n",
    "if np.max(edge_weights) != np.min(edge_weights):\n",
    "    norm_edge_weights = (edge_weights - np.min(edge_weights)) / (np.max(edge_weights) - np.min(edge_weights))\n",
    "else:\n",
    "    norm_edge_weights = np.zeros_like(edge_weights)\n",
    "\n",
    "cmap = cm.get_cmap('hot')\n",
    "edge_colors = cmap(norm_edge_weights)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "node_xyz = np.array([coords[i] for i in G_3d.nodes()])\n",
    "\n",
    "ax.scatter(node_xyz[:, 0], node_xyz[:, 1], node_xyz[:, 2], s=node_sizes, c='hot', alpha=0.9)\n",
    "\n",
    "for i, (u, v) in enumerate(G_3d.edges()):\n",
    "    x = np.array([coords[u, 0], coords[v, 0]])\n",
    "    y = np.array([coords[u, 1], coords[v, 1]])\n",
    "    z = np.array([coords[u, 2], coords[v, 2]])\n",
    "    ax.plot(x, y, z, color=edge_colors[i], alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.title('Gráfico 3D de la matriz de coactivación')\n",
    "\n",
    "plt.show()"
   ],
   "id": "ce54618d239ad97a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EJEERCICIO 4",
   "id": "164ed0575c6ab19b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import mne\n",
    "raw = mne.io.read_raw_edf(r\"C:/Users/rayom/OneDrive/Documentos/GitHub/Neurociencias-2026-1/S03_datasets/EEG_practica\", preload=True)\n",
    "\n",
    "print(raw)\n",
    "print(raw.info)"
   ],
   "id": "d6fe7303c2006007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import bct\n",
    "\n",
    "eeg_channels = [ch for ch in raw.ch_names if 'EEG' in ch];\n",
    "raw_eeg = raw.copy().pick_channels(eeg_channels, ordered=True);\n",
    "variances = raw_eeg.get_data().var(axis=1);\n",
    "variance_threshold = 1e-10\n",
    "low_variance_channels = [raw_eeg.ch_names[i] for i, var in enumerate(variances) if var < variance_threshold];\n",
    "raw_eeg.drop_channels(low_variance_channels);\n",
    "raw_eeg.set_eeg_reference('average', projection=True);\n",
    "raw_eeg.apply_proj();\n",
    "l_freq, h_freq = 1, 40\n",
    "raw_eeg.filter(l_freq, h_freq, fir_design='firwin');\n",
    "preprocessed_data = raw_eeg.get_data();\n",
    "\n",
    "connectivity_matrix = np.corrcoef(preprocessed_data);\n",
    "connectivity_matrix_df = pd.DataFrame(connectivity_matrix);\n",
    "connectivity_matrix_df.fillna(0, inplace=True);\n",
    "connectivity_matrix_df.replace([np.inf, -np.inf], 0, inplace=True);\n",
    "\n",
    "G_edf = nx.from_numpy_array(connectivity_matrix_df.values);\n",
    "\n",
    "\n",
    "# nodos\n",
    "edf_degrees = dict(G_edf.degree())\n",
    "print(\"Grado de nodos:\", edf_degrees)\n",
    "\n",
    "# Eficiencia global\n",
    "edf_global_efficiency = bct.efficiency_wei(connectivity_matrix_df.values)\n",
    "print(\"Eficiencia Global:\", edf_global_efficiency)\n",
    "\n",
    "# Eficienciea local\n",
    "edf_local_efficiency = bct.efficiency_wei(connectivity_matrix_df.values, local=True)\n",
    "print(\"Eficiencia local:\", edf_local_efficiency)\n",
    "\n",
    "# Cluster\n",
    "edf_clustering_coefficient = nx.average_clustering(G_edf)\n",
    "print(\"Coeficiente de Cluster:\", edf_clustering_coefficient)\n",
    "\n",
    "# Ruta corta\n",
    "if nx.is_connected(G_edf):\n",
    "    edf_avg_shortest_path_length = nx.average_shortest_path_length(G_edf)\n",
    "    print(\"Ruta mas corta:\", edf_avg_shortest_path_length)\n",
    "else:\n",
    "    print(\"Gráfica desconectada.\")\n",
    "    largest_component_edf = max(nx.connected_components(G_edf), key=len)\n",
    "    G_edf_largest_component = G_edf.subgraph(largest_component_edf).copy()\n",
    "    if len(G_edf_largest_component) > 1:\n",
    "        edf_avg_shortest_path_length = nx.average_shortest_path_length(G_edf_largest_component)\n",
    "        print(\"Promedio camino mas corto\", edf_avg_shortest_path_length)\n",
    "    else:\n",
    "        edf_avg_shortest_path_length = 0\n",
    "        print(\"El componente mas largo solo tiene un nodo\")\n",
    "\n",
    "# Mundo Pequeño\n",
    "n_nodes_edf = G_edf.number_of_nodes()\n",
    "n_edges_edf = G_edf.number_of_edges()\n",
    "G_random_edf = nx.gnm_random_graph(n_nodes_edf, n_edges_edf)\n",
    "clustering_coefficient_random_edf = nx.average_clustering(G_random_edf)\n",
    "print(\"Coeficiente de cluster\", clustering_coefficient_random_edf)\n",
    "if nx.is_connected(G_random_edf):\n",
    "    avg_shortest_path_length_random_edf = nx.average_shortest_path_length(G_random_edf)\n",
    "    print(\"Camino mas corto\", avg_shortest_path_length_random_edf)\n",
    "else:\n",
    "    print(\"La grafica esta desconectada.\")\n",
    "    largest_component_random_edf = max(nx.connected_components(G_random_edf), key=len)\n",
    "    G_random_largest_component_edf = G_random_edf.subgraph(largest_component_random_edf).copy()\n",
    "    if len(G_random_largest_component_edf) > 1:\n",
    "        avg_shortest_path_length_random_edf = nx.average_shortest_path_length(G_random_largest_component_edf)\n",
    "        print(\"Average Shortest Path Length (Random Graph - Largest Component):\", avg_shortest_path_length_random_edf)\n",
    "    else:\n",
    "        avg_shortest_path_length_random_edf = 0\n",
    "        print(\"Largest component of random graph has only one node. Average shortest path length is undefined.\")\n",
    "\n",
    "if avg_shortest_path_length_random_edf > 0 and edf_avg_shortest_path_length > 0 and clustering_coefficient_random_edf > 0:\n",
    "    small_world_coefficient_edf = (edf_clustering_coefficient / clustering_coefficient_random_edf) / (edf_avg_shortest_path_length / avg_shortest_path_length_random_edf)\n",
    "    print(\"Small-world Coefficient:\", small_world_coefficient_edf)\n",
    "elif clustering_coefficient_random_edf == 0:\n",
    "     print(\"Clustering coefficient of random graph is zero. Cannot calculate small-world coefficient.\")\n",
    "else:\n",
    "    print(\"Average shortest path length of original or random graph is zero or undefined. Cannot calculate small-world coefficient.\")\n",
    "\n",
    "# Comunidades\n",
    "G_edf_abs = nx.from_numpy_array(np.abs(connectivity_matrix_df.values))\n",
    "edf_partition = community_louvain.best_partition(G_edf_abs)\n",
    "print(\"Communities:\", edf_partition)\n",
    "\n",
    "#hubs\n",
    "edf_degrees_abs = dict(G_edf_abs.degree())\n",
    "edf_avg_degree_abs = np.mean(list(edf_degrees_abs.values()))\n",
    "edf_hubs_abs = [node for node, degree in edf_degrees_abs.items() if degree > edf_avg_degree_abs]\n",
    "print(\"Potential Hubs (degree > average) using absolute weights:\", edf_hubs_abs)"
   ],
   "id": "33c0605770925508",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EJERCICIO 5",
   "id": "27cb8b9a681d3e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "num_nodes = G_edf.number_of_nodes()\n",
    "\n",
    "np.random.seed(42)\n",
    "coords_edf = np.random.rand(num_nodes, 3) * 10\n",
    "\n",
    "edf_degrees = dict(G_edf.degree())\n",
    "node_size_factor = 50\n",
    "node_sizes_edf = [edf_degrees[node] * node_size_factor for node in G_edf.nodes()]\n",
    "\n",
    "edge_weights_edf = np.array([G_edf[u][v]['weight'] for u, v in G_edf.edges()])\n",
    "abs_edge_weights_edf = np.abs(edge_weights_edf)\n",
    "\n",
    "\n",
    "if np.max(abs_edge_weights_edf) != np.min(abs_edge_weights_edf):\n",
    "    norm_edge_colors_edf = (abs_edge_weights_edf - np.min(abs_edge_weights_edf)) / (np.max(abs_edge_weights_edf) - np.min(abs_edge_weights_edf))\n",
    "else:\n",
    "    norm_edge_colors_edf = np.zeros_like(abs_edge_weights_edf) # All weights are the same, use one color\n",
    "\n",
    "cmap = cm.get_cmap('hot')\n",
    "edge_colors_edf = cmap(norm_edge_colors_edf)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "node_xyz_edf = coords_edf\n",
    "\n",
    "ax.scatter(node_xyz_edf[:, 0], node_xyz_edf[:, 1], node_xyz_edf[:, 2], s=node_sizes_edf, c='blue', alpha=0.8)\n",
    "\n",
    "for i, (u, v) in enumerate(G_edf.edges()):\n",
    "    x = np.array([coords_edf[u, 0], coords_edf[v, 0]])\n",
    "    y = np.array([coords_edf[u, 1], coords_edf[v, 1]])\n",
    "    z = np.array([coords_edf[u, 2], coords_edf[v, 2]])\n",
    "    ax.plot(x, y, z, color=edge_colors_edf[i], alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.title('Grafico 3D de EEG de la matriz de conectividad')"
   ],
   "id": "3c3c7d1c5366b350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fca4e538cab0e82",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
